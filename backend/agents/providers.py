from abc import ABC, abstractmethod
from typing import Dict, Any, List, AsyncGenerator, Callable, Type
from pydantic import BaseModel
import json
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from config import OPENAI_API_KEY, ANTHROPIC_API_KEY
import base64
import httpx


class AgentTool(BaseModel):
    name: str
    description: str
    parameters: Dict[str, Any]
    func: Callable

    def to_oai_tool(self):
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": self.parameters,
            },
        }

    def to_anthropic_tool(self):
        return {
            "name": self.name,
            "description": self.description,
            "input_schema": {
                "type": "object",
                "properties": self.parameters["properties"],
                "required": self.parameters.get("required", []),
            },
        }


class LLMProvider(ABC):
    @abstractmethod
    async def chat_complete(
        self, system_prompt: str, user_prompt: str, model: str, temperature: float = 0.0
    ) -> str:
        pass

    @abstractmethod
    async def chat_complete_with_tools(
        self,
        messages: List[Dict[str, Any]],
        tools: List[AgentTool],
        model: str,
        temperature: float = 0.0,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        pass


class OpenAILLMProvider(LLMProvider):
    def __init__(self):
        self.client = AsyncOpenAI(api_key=OPENAI_API_KEY)

    async def chat_complete(
        self, system_prompt: str, user_prompt: str, model: str, temperature: float = 0.0
    ) -> str:
        resp = await self.client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=temperature,
        )
        return resp.choices[0].message.content

    async def _handle_tool_call(self, tools: List[AgentTool], tool_call) -> str:
        # Default implementation for OpenAI format
        tool_name = tool_call.function.name
        arguments = json.loads(tool_call.function.arguments)

        tool = next((tool for tool in tools if tool.name == tool_name), None)
        if not tool:
            raise ValueError(f"Unknown tool: {tool_name}")
        return await tool.func(**arguments)

    async def chat_complete_with_tools(
        self,
        messages: List[Dict[str, Any]],
        tools: List[AgentTool],
        model: str,
        temperature: float = 0.0,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        running = True
        oai_messages = messages.copy()

        while running:
            # Only include tools parameter if tools are provided
            create_params = {
                "model": model,
                "messages": oai_messages,
                "temperature": temperature,
                "stream": True,
            }

            if tools:
                create_params["tools"] = [tool.to_oai_tool() for tool in tools]

            stream = await self.client.chat.completions.create(**create_params)

            tool_calls_buffer = []
            current_tool_call = None

            async for chunk in stream:
                delta = chunk.choices[0].delta
                finish_reason = chunk.choices[0].finish_reason

                if delta.tool_calls:
                    for tool_call_delta in delta.tool_calls:
                        if tool_call_delta.index is not None:
                            if len(tool_calls_buffer) <= tool_call_delta.index:
                                tool_calls_buffer.append(tool_call_delta)
                                current_tool_call = tool_calls_buffer[
                                    tool_call_delta.index
                                ]
                            else:
                                current_tool_call = tool_calls_buffer[
                                    tool_call_delta.index
                                ]
                                if tool_call_delta.function.name:
                                    current_tool_call.function.name = (
                                        tool_call_delta.function.name
                                    )
                                if tool_call_delta.function.arguments:
                                    if not hasattr(
                                        current_tool_call.function, "arguments"
                                    ):
                                        current_tool_call.function.arguments = ""
                                    current_tool_call.function.arguments += (
                                        tool_call_delta.function.arguments
                                    )

                if finish_reason == "tool_calls":
                    oai_messages.append(
                        {
                            "role": "assistant",
                            "content": None,
                            "tool_calls": tool_calls_buffer,
                        }
                    )
                    for tool_call in tool_calls_buffer:
                        tool_result = await self._handle_tool_call(tools, tool_call)
                        oai_messages.append(
                            {
                                "role": "tool",
                                "content": tool_result,
                                "name": tool_call.function.name,
                                "tool_call_id": tool_call.id,
                            }
                        )
                    yield {"type": "tool_calls", "tool_calls": tool_calls_buffer}
                elif finish_reason == "stop":
                    running = False
                    break

                if delta.content is not None:
                    yield {"type": "content", "content": delta.content}


class AnthropicLLMProvider(LLMProvider):
    def __init__(self):
        self.client = AsyncAnthropic(api_key=ANTHROPIC_API_KEY)
        # Create a shared httpx client for image fetching
        self.http_client = httpx.AsyncClient()

    async def _fetch_and_encode_image(self, url: str) -> tuple[str, str]:
        """Fetch image from URL and return (media_type, base64_data)"""
        resp = await self.http_client.get(url)
        resp.raise_for_status()
        media_type = resp.headers.get("content-type", "image/jpeg")
        b64_data = base64.b64encode(resp.content).decode("utf-8")
        return media_type, b64_data

    async def chat_complete(
        self, system_prompt: str, user_prompt: str, model: str, temperature: float = 0.0
    ) -> str:
        response = await self.client.messages.create(
            model=model,
            messages=[{"role": "user", "content": user_prompt}],
            system=system_prompt,
            temperature=temperature,
            max_tokens=8192,
        )
        return response.content[0].text

    async def chat_complete_with_tools(
        self,
        messages: List[Dict[str, Any]],
        tools: List[AgentTool],
        model: str,
        temperature: float = 0.0,
    ) -> AsyncGenerator[Dict[str, Any], None]:
        try:
            # Convert tools to Anthropic format
            anthropic_tools = [tool.to_anthropic_tool() for tool in tools]

            # Extract system message and prepare current messages
            system_message = next(
                (msg["content"] for msg in messages if msg["role"] == "system"), None
            )

            # Convert messages to Anthropic format with image support
            current_messages = []
            for msg in messages:
                if msg["role"] == "system":
                    continue

                content = []
                if isinstance(msg["content"], list):
                    for content_block in msg["content"]:
                        if content_block["type"] == "text":
                            # Only add text block if it's not empty
                            if content_block["text"].strip():
                                content.append(
                                    {"type": "text", "text": content_block["text"]}
                                )
                        elif content_block["type"] == "image_url":
                            # Fetch and encode the image
                            media_type, b64_data = await self._fetch_and_encode_image(
                                content_block["image_url"]["url"]
                            )
                            content.append(
                                {
                                    "type": "image",
                                    "source": {
                                        "type": "base64",
                                        "media_type": media_type,
                                        "data": b64_data,
                                    },
                                }
                            )
                else:
                    # Only add text content if it's not empty
                    if msg["content"] and msg["content"].strip():
                        content.append({"type": "text", "text": msg["content"]})

                # Only add message if it has content
                if content:
                    current_messages.append({"role": msg["role"], "content": content})

            running = True
            while running:
                create_params = {
                    "model": model,
                    "messages": current_messages,
                    "system": system_message,
                    "temperature": temperature,
                    "stream": True,
                    "max_tokens": 8192,
                }

                if tools:
                    create_params.update(
                        {"tools": anthropic_tools, "tool_choice": {"type": "auto"}}
                    )

                stream = await self.client.messages.create(**create_params)

                tool_calls_buffer = []
                content_buffer = ""

                async for chunk in stream:
                    if chunk.type == "message_start":
                        continue

                    if chunk.type == "content_block_start":
                        if (
                            hasattr(chunk.content_block, "type")
                            and chunk.content_block.type == "tool_use"
                        ):
                            # Add new tool call to buffer
                            tool_calls_buffer.append(
                                {
                                    "id": chunk.content_block.id,
                                    "function": {
                                        "name": chunk.content_block.name,
                                        "arguments": "",
                                    },
                                }
                            )

                    elif chunk.type == "content_block_delta":
                        if hasattr(chunk.delta, "type"):
                            if chunk.delta.type == "text_delta":
                                content_buffer += chunk.delta.text
                                yield {"type": "content", "content": chunk.delta.text}
                            elif chunk.delta.type == "input_json_delta":
                                # Update arguments for the last tool call in buffer
                                if tool_calls_buffer:
                                    tool_calls_buffer[-1]["function"][
                                        "arguments"
                                    ] += chunk.delta.partial_json

                    elif chunk.type == "content_block_stop":
                        if tool_calls_buffer:
                            # Process all tool calls in buffer
                            for tool_call in tool_calls_buffer:

                                arguments_dict = json.loads(
                                    tool_call["function"]["arguments"]
                                )
                                tool_result = await self._handle_tool_call(
                                    tools, tool_call
                                )

                                # Add tool use message
                                current_messages.append(
                                    {
                                        "role": "assistant",
                                        "content": [
                                            {
                                                "type": "tool_use",
                                                "id": tool_call["id"],
                                                "name": tool_call["function"]["name"],
                                                "input": arguments_dict,
                                            }
                                        ],
                                    }
                                )

                                # Add tool result message
                                current_messages.append(
                                    {
                                        "role": "user",
                                        "content": [
                                            {
                                                "type": "tool_result",
                                                "tool_use_id": tool_call["id"],
                                                "content": tool_result,
                                            }
                                        ],
                                    }
                                )

                            # Yield all tool calls at once
                            yield {
                                "type": "tool_calls",
                                "tool_calls": tool_calls_buffer,
                            }
                            tool_calls_buffer = []
                            content_buffer = ""
                        else:
                            running = False
                            break

        finally:
            # Ensure we clean up the HTTP client
            await self.http_client.aclose()

    async def _handle_tool_call(self, tools: List[AgentTool], tool_call) -> str:
        # Anthropic specific implementation
        tool_name = tool_call["function"]["name"]
        arguments = json.loads(tool_call["function"]["arguments"])

        tool = next((tool for tool in tools if tool.name == tool_name), None)
        if not tool:
            raise ValueError(f"Unknown tool: {tool_name}")
        return await tool.func(**arguments)


LLM_PROVIDERS: Dict[str, Type[LLMProvider]] = {
    "openai": OpenAILLMProvider,
    "anthropic": AnthropicLLMProvider,
}
